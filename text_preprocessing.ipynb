{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"E:/NLP/text_preprocessing/data/twcs/twcs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>inbound</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>response_tweet_id</th>\n",
       "      <th>in_response_to_tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>sprintcare</td>\n",
       "      <td>False</td>\n",
       "      <td>Tue Oct 31 22:10:47 +0000 2017</td>\n",
       "      <td>@115712 I understand. I would like to assist y...</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>115712</td>\n",
       "      <td>True</td>\n",
       "      <td>Tue Oct 31 22:11:45 +0000 2017</td>\n",
       "      <td>@sprintcare and how do you propose we do that</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>115712</td>\n",
       "      <td>True</td>\n",
       "      <td>Tue Oct 31 22:08:27 +0000 2017</td>\n",
       "      <td>@sprintcare I have sent several private messag...</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>sprintcare</td>\n",
       "      <td>False</td>\n",
       "      <td>Tue Oct 31 21:54:49 +0000 2017</td>\n",
       "      <td>@115712 Please send us a Private Message so th...</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>115712</td>\n",
       "      <td>True</td>\n",
       "      <td>Tue Oct 31 21:49:35 +0000 2017</td>\n",
       "      <td>@sprintcare I did.</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id   author_id  inbound                      created_at  \\\n",
       "0         1  sprintcare    False  Tue Oct 31 22:10:47 +0000 2017   \n",
       "1         2      115712     True  Tue Oct 31 22:11:45 +0000 2017   \n",
       "2         3      115712     True  Tue Oct 31 22:08:27 +0000 2017   \n",
       "3         4  sprintcare    False  Tue Oct 31 21:54:49 +0000 2017   \n",
       "4         5      115712     True  Tue Oct 31 21:49:35 +0000 2017   \n",
       "\n",
       "                                                text response_tweet_id  \\\n",
       "0  @115712 I understand. I would like to assist y...                 2   \n",
       "1      @sprintcare and how do you propose we do that               NaN   \n",
       "2  @sprintcare I have sent several private messag...                 1   \n",
       "3  @115712 Please send us a Private Message so th...                 3   \n",
       "4                                 @sprintcare I did.                 4   \n",
       "\n",
       "   in_response_to_tweet_id  \n",
       "0                      3.0  \n",
       "1                      1.0  \n",
       "2                      4.0  \n",
       "3                      5.0  \n",
       "4                      6.0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2811774 entries, 0 to 2811773\n",
      "Data columns (total 7 columns):\n",
      " #   Column                   Dtype  \n",
      "---  ------                   -----  \n",
      " 0   tweet_id                 int64  \n",
      " 1   author_id                object \n",
      " 2   inbound                  bool   \n",
      " 3   created_at               object \n",
      " 4   text                     object \n",
      " 5   response_tweet_id        object \n",
      " 6   in_response_to_tweet_id  float64\n",
      "dtypes: bool(1), float64(1), int64(1), object(4)\n",
      "memory usage: 131.4+ MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2811774, 7)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since the full data taking too much time(approx 200 minutes and still running) to do \n",
    "# stemming and lemmatization process, that's why took sample size of 100k for process\n",
    "random_data=data.sample(n=100000,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 7)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_data=random_data[['tweet_id','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>160535</th>\n",
       "      <td>192624</td>\n",
       "      <td>@161252 What's that egg website people talk about</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659248</th>\n",
       "      <td>738238</td>\n",
       "      <td>Why!ü§∑üèª‚Äç‚ôÄÔ∏è #iOS11 @AppleSupport https://t.co/BX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2250310</th>\n",
       "      <td>2414302</td>\n",
       "      <td>@693975 We can assist you. We recommend updati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640680</th>\n",
       "      <td>1793929</td>\n",
       "      <td>@331912 @115955 Thats better than having an un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933623</th>\n",
       "      <td>2088018</td>\n",
       "      <td>@VirginAmerica is probably one of the best air...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text\n",
       "160535     192624  @161252 What's that egg website people talk about\n",
       "659248     738238  Why!ü§∑üèª‚Äç‚ôÄÔ∏è #iOS11 @AppleSupport https://t.co/BX...\n",
       "2250310   2414302  @693975 We can assist you. We recommend updati...\n",
       "1640680   1793929  @331912 @115955 Thats better than having an un...\n",
       "1933623   2088018  @VirginAmerica is probably one of the best air..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id    0\n",
       "text        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "port_stemmer=PorterStemmer()\n",
    "\n",
    "def stemming(content):\n",
    "    stemmed_content=re.sub('[^a-zA-Z]',' ',content)\n",
    "    stemmed_content=stemmed_content.lower()\n",
    "    stemmed_content=stemmed_content.split()\n",
    "    stemmed_content=[port_stemmer.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "    stemmed_content=' '.join(stemmed_content)\n",
    "    return stemmed_content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_data['stemmed_content']=random_data['text'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>stemmed_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>160535</th>\n",
       "      <td>192624</td>\n",
       "      <td>@161252 What's that egg website people talk about</td>\n",
       "      <td>egg websit peopl talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659248</th>\n",
       "      <td>738238</td>\n",
       "      <td>Why!ü§∑üèª‚Äç‚ôÄÔ∏è #iOS11 @AppleSupport https://t.co/BX...</td>\n",
       "      <td>io applesupport http co bxrvfeixxq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2250310</th>\n",
       "      <td>2414302</td>\n",
       "      <td>@693975 We can assist you. We recommend updati...</td>\n",
       "      <td>assist recommend updat io chanc also dm us fol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640680</th>\n",
       "      <td>1793929</td>\n",
       "      <td>@331912 @115955 Thats better than having an un...</td>\n",
       "      <td>that better unstabl connect drop everi min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933623</th>\n",
       "      <td>2088018</td>\n",
       "      <td>@VirginAmerica is probably one of the best air...</td>\n",
       "      <td>virginamerica probabl one best airlin ever exp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "160535     192624  @161252 What's that egg website people talk about   \n",
       "659248     738238  Why!ü§∑üèª‚Äç‚ôÄÔ∏è #iOS11 @AppleSupport https://t.co/BX...   \n",
       "2250310   2414302  @693975 We can assist you. We recommend updati...   \n",
       "1640680   1793929  @331912 @115955 Thats better than having an un...   \n",
       "1933623   2088018  @VirginAmerica is probably one of the best air...   \n",
       "\n",
       "                                           stemmed_content  \n",
       "160535                               egg websit peopl talk  \n",
       "659248                  io applesupport http co bxrvfeixxq  \n",
       "2250310  assist recommend updat io chanc also dm us fol...  \n",
       "1640680         that better unstabl connect drop everi min  \n",
       "1933623  virginamerica probabl one best airlin ever exp...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def lemmatization(content):\n",
    "    cleaned_content = re.sub('[^a-zA-Z]', ' ', content)\n",
    "    cleaned_content = cleaned_content.lower()\n",
    "    tokens = cleaned_content.split()\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    # Apply lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    lemmatized_content = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_data['lemmatized_content']=random_data['text'].apply(lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>stemmed_content</th>\n",
       "      <th>lemmatized_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>160535</th>\n",
       "      <td>192624</td>\n",
       "      <td>@161252 What's that egg website people talk about</td>\n",
       "      <td>egg websit peopl talk</td>\n",
       "      <td>egg website people talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659248</th>\n",
       "      <td>738238</td>\n",
       "      <td>Why!ü§∑üèª‚Äç‚ôÄÔ∏è #iOS11 @AppleSupport https://t.co/BX...</td>\n",
       "      <td>io applesupport http co bxrvfeixxq</td>\n",
       "      <td>io applesupport http co bxrvfeixxq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2250310</th>\n",
       "      <td>2414302</td>\n",
       "      <td>@693975 We can assist you. We recommend updati...</td>\n",
       "      <td>assist recommend updat io chanc also dm us fol...</td>\n",
       "      <td>assist recommend updating io chance also dm u ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640680</th>\n",
       "      <td>1793929</td>\n",
       "      <td>@331912 @115955 Thats better than having an un...</td>\n",
       "      <td>that better unstabl connect drop everi min</td>\n",
       "      <td>thats better unstable connection drop every min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933623</th>\n",
       "      <td>2088018</td>\n",
       "      <td>@VirginAmerica is probably one of the best air...</td>\n",
       "      <td>virginamerica probabl one best airlin ever exp...</td>\n",
       "      <td>virginamerica probably one best airline ever e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "160535     192624  @161252 What's that egg website people talk about   \n",
       "659248     738238  Why!ü§∑üèª‚Äç‚ôÄÔ∏è #iOS11 @AppleSupport https://t.co/BX...   \n",
       "2250310   2414302  @693975 We can assist you. We recommend updati...   \n",
       "1640680   1793929  @331912 @115955 Thats better than having an un...   \n",
       "1933623   2088018  @VirginAmerica is probably one of the best air...   \n",
       "\n",
       "                                           stemmed_content  \\\n",
       "160535                               egg websit peopl talk   \n",
       "659248                  io applesupport http co bxrvfeixxq   \n",
       "2250310  assist recommend updat io chanc also dm us fol...   \n",
       "1640680         that better unstabl connect drop everi min   \n",
       "1933623  virginamerica probabl one best airlin ever exp...   \n",
       "\n",
       "                                        lemmatized_content  \n",
       "160535                             egg website people talk  \n",
       "659248                  io applesupport http co bxrvfeixxq  \n",
       "2250310  assist recommend updating io chance also dm u ...  \n",
       "1640680    thats better unstable connection drop every min  \n",
       "1933623  virginamerica probably one best airline ever e...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "final_data=copy.deepcopy(random_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>stemmed_content</th>\n",
       "      <th>lemmatized_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>160535</th>\n",
       "      <td>192624</td>\n",
       "      <td>@161252 What's that egg website people talk about</td>\n",
       "      <td>egg websit peopl talk</td>\n",
       "      <td>egg website people talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659248</th>\n",
       "      <td>738238</td>\n",
       "      <td>Why!ü§∑üèª‚Äç‚ôÄÔ∏è #iOS11 @AppleSupport https://t.co/BX...</td>\n",
       "      <td>io applesupport http co bxrvfeixxq</td>\n",
       "      <td>io applesupport http co bxrvfeixxq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2250310</th>\n",
       "      <td>2414302</td>\n",
       "      <td>@693975 We can assist you. We recommend updati...</td>\n",
       "      <td>assist recommend updat io chanc also dm us fol...</td>\n",
       "      <td>assist recommend updating io chance also dm u ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640680</th>\n",
       "      <td>1793929</td>\n",
       "      <td>@331912 @115955 Thats better than having an un...</td>\n",
       "      <td>that better unstabl connect drop everi min</td>\n",
       "      <td>thats better unstable connection drop every min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933623</th>\n",
       "      <td>2088018</td>\n",
       "      <td>@VirginAmerica is probably one of the best air...</td>\n",
       "      <td>virginamerica probabl one best airlin ever exp...</td>\n",
       "      <td>virginamerica probably one best airline ever e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "160535     192624  @161252 What's that egg website people talk about   \n",
       "659248     738238  Why!ü§∑üèª‚Äç‚ôÄÔ∏è #iOS11 @AppleSupport https://t.co/BX...   \n",
       "2250310   2414302  @693975 We can assist you. We recommend updati...   \n",
       "1640680   1793929  @331912 @115955 Thats better than having an un...   \n",
       "1933623   2088018  @VirginAmerica is probably one of the best air...   \n",
       "\n",
       "                                           stemmed_content  \\\n",
       "160535                               egg websit peopl talk   \n",
       "659248                  io applesupport http co bxrvfeixxq   \n",
       "2250310  assist recommend updat io chanc also dm us fol...   \n",
       "1640680         that better unstabl connect drop everi min   \n",
       "1933623  virginamerica probabl one best airlin ever exp...   \n",
       "\n",
       "                                        lemmatized_content  \n",
       "160535                             egg website people talk  \n",
       "659248                  io applesupport http co bxrvfeixxq  \n",
       "2250310  assist recommend updating io chance also dm u ...  \n",
       "1640680    thats better unstable connection drop every min  \n",
       "1933623  virginamerica probably one best airline ever e...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#since while taking all the 100k data i am getting memory error\n",
    "#MemoryError: Unable to allocate 124. GiB for an array with shape (100000, 83200) and data type int64\n",
    "#that's why reducung the feature numbers\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=3000)\n",
    "one_hot_encoded = vectorizer.fit_transform(final_data['lemmatized_content'])\n",
    "\n",
    "# Convert the result to a dense NumPy array\n",
    "one_hot_encoded_array = one_hot_encoded.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(len(one_hot_encoded_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in e:\\nlp\\text_preprocessing\\data\\twittervenv\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in e:\\nlp\\text_preprocessing\\data\\twittervenv\\lib\\site-packages (from scikit-learn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in e:\\nlp\\text_preprocessing\\data\\twittervenv\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in e:\\nlp\\text_preprocessing\\data\\twittervenv\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in e:\\nlp\\text_preprocessing\\data\\twittervenv\\lib\\site-packages (from scikit-learn) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=3000)\n",
    "tfidf_encoded = tfidf_vectorizer.fit_transform(final_data['lemmatized_content'])\n",
    "\n",
    "# Convert the result to a dense NumPy array\n",
    "tfidf_encoded_array = tfidf_encoded.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_encoded_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 345. GiB for an array with shape (100000, 463248) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m bi_gram_vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m      5\u001b[0m bi_gram_encoded \u001b[38;5;241m=\u001b[39m bi_gram_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(final_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmatized_content\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 6\u001b[0m bi_gram_encoded_array \u001b[38;5;241m=\u001b[39m \u001b[43mbi_gram_encoded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBi-grams:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(bi_gram_encoded_array)\n",
      "File \u001b[1;32me:\\NLP\\text_preprocessing\\data\\twittervenv\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1056\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1055\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1056\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32me:\\NLP\\text_preprocessing\\data\\twittervenv\\lib\\site-packages\\scipy\\sparse\\_base.py:1287\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 345. GiB for an array with shape (100000, 463248) and data type int64"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Bi-grams\n",
    "bi_gram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "bi_gram_encoded = bi_gram_vectorizer.fit_transform(final_data['lemmatized_content'])\n",
    "bi_gram_encoded_array = bi_gram_encoded.toarray()\n",
    "\n",
    "print(\"Bi-grams:\")\n",
    "print(bi_gram_encoded_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigrams\n",
    "tri_gram_vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "tri_gram_encoded = tri_gram_vectorizer.fit_transform(final_data['lemmatized_content'])\n",
    "tri_gram_encoded_array = tri_gram_encoded.toarray()\n",
    "\n",
    "print(\"\\nTrigrams:\")\n",
    "print(tri_gram_encoded_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_encoded = tfidf_vectorizer.fit_transform(random_data)\n",
    "tfidf_encoded_array = tfidf_encoded.toarray()\n",
    "tfidf_df = pd.DataFrame(tfidf_encoded_array, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"\\nTF-IDF:\")\n",
    "print(tfidf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
